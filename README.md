# Internal Knowledge Assistant (LLM)

A production-oriented LLM system for internal knowledge retrieval, grounded Q&A, and safety-aware responses.  
This project focuses on prompt design, evaluation, retrieval (RAG), and guardrails, with an extension to customer support use cases.

---

## ğŸ¯ Project Goals

- Enable accurate and grounded answers from internal documents
- Minimize hallucinations through retrieval-based prompting
- Provide safe and predictable responses with clear refusal and escalation rules
- Demonstrate production-oriented prompt engineering practices

---

## ğŸ§  System Overview

This project is designed as an **Internal Knowledge Assistant**, where an LLM answers questions **only based on provided internal documents**.

Key characteristics:
- Retrieval-Augmented Generation (RAG)
- Explicit refusal when information is missing
- Safety and escalation guardrails
- Prompt evaluation using structured test cases

The same core system can be extended to external-facing use cases such as customer support by modifying tone, rules, and escalation logic.

---

## ğŸ—‚ Repository Structure

â”œâ”€â”€ core/
â”‚ â”œâ”€â”€ system_prompt.md # Global system behavior
â”‚ â”œâ”€â”€ retrieval_prompt.md # Context-grounded QA rules
â”‚ â””â”€â”€ refusal_policy.md # Safety & refusal guidelines
â”œâ”€â”€ domains/
â”‚ â”œâ”€â”€ internal_knowledge/
â”‚ â”‚ â”œâ”€â”€ qa_prompt.md
â”‚ â”‚ â”œâ”€â”€ examples.md
â”‚ â”‚ â””â”€â”€ test_cases.json
â”‚ â””â”€â”€ customer_support/
â”‚ â”œâ”€â”€ support_prompt.md
â”‚ â”œâ”€â”€ escalation_rules.md
â”‚ â””â”€â”€ test_cases.json
â”œâ”€â”€ data/
â”‚ â””â”€â”€ internal_docs/ # Sample internal documents (markdown)
â”œâ”€â”€ eval/
â”‚ â”œâ”€â”€ rubric.md # Evaluation criteria
â”‚ â””â”€â”€ evaluation_notes.md
â””â”€â”€ README.md

---

## âœï¸ Prompt Design Principles

- **Accuracy over fluency**  
  The assistant prioritizes correctness and grounded answers over sounding helpful.

- **Context-bound responses**  
  Answers must be derived strictly from retrieved documents.

- **Explicit uncertainty**  
  If information is missing, the assistant clearly states it does not know.

- **Separation of concerns**  
  System behavior, task prompts, and safety rules are modular and reusable.

---

## ğŸ“Š Evaluation Strategy

Prompt quality is evaluated using predefined test cases and qualitative rubrics.

Each test case specifies:
- User input
- Expected behavior
- Tone constraints
- Forbidden responses (e.g. hallucinations, promises, legal advice)

Evaluation dimensions:
- Accuracy
- Completeness
- Tone & professionalism
- Safety & refusal correctness

See:
- `eval/rubric.md`
- `domains/*/test_cases.json`

---

## ğŸ›¡ Safety & Guardrails

The assistant enforces guardrails for:
- Missing or insufficient context
- Legal, financial, or policy-sensitive requests
- Aggressive or abusive language
- Requests requiring human escalation

Unsafe or unsupported requests trigger polite refusals or escalation cues instead of speculative answers.

---

## ğŸ” Extension: Customer Support Assistant

The internal knowledge system is extended into a customer support assistant by:
- Adjusting tone and response style
- Adding escalation rules
- Reusing the same retrieval and safety foundations

This demonstrates how a single LLM architecture can serve multiple domains with minimal changes.

---

## ğŸš€ Future Improvements

- Automated evaluation scripts
- Citation-level answer grounding
- Confidence scoring for responses
- Agent-based document routing

---

## ğŸ“Œ Notes

This repository is intended as a **portfolio project** to demonstrate applied LLM prompt engineering, evaluation thinking, and production awareness.
